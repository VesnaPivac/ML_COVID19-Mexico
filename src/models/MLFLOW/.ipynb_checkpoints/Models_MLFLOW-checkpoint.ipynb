{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelos aplicando MLFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Importar librería para cargar datos procesados\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Importar modelos de Machine Learning\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Importar funciones para el entrenamiento y validación de los modelos\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, log_loss\n",
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "#Importa MLFLOW\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from mlflow.models import infer_signature\n",
    "\n",
    "import json\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carga de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Datos de entrenamiento\n",
    "X_train = pd.read_parquet('../../../data/processed/X_train.parquet')\n",
    "y_train = pd.read_parquet('../../../data/processed/y_train.parquet')\n",
    "\n",
    "# Datos de prueba\n",
    "X_test = pd.read_parquet('../../../data/processed/X_test.parquet')\n",
    "y_test= pd.read_parquet('../../../data/processed/y_test.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (940754, 44)\n",
      "y_train: (940754, 1)\n",
      "X_test:  (235189, 44)\n",
      "y_test:  (235189, 1)\n"
     ]
    }
   ],
   "source": [
    "print('X_train:',X_train.shape)\n",
    "print('y_train:',y_train.shape)\n",
    "print('X_test: ',X_test.shape)\n",
    "print('y_test: ',y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLFLOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "track_uri = \"http://localhost:8080/\" # Esto puede ser que cambie por http://0.0.0.0:1234\n",
    "mlflow.set_tracking_uri(track_uri)\n",
    "mlflow.set_registry_uri(\"sqlite:////tmp/registry.db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLflow Version: 2.13.0\n",
      "Tracking URI: http://localhost:8080/\n",
      "Nombre del experimento: ML_COVID19-MEXICO\n",
      "ID del experimento: 707578369122819239\n"
     ]
    }
   ],
   "source": [
    "# Generando el experimento o cargandolo si existe\n",
    "experiment_name = \"ML_COVID19-MEXICO\"\n",
    "mlflow.set_experiment(experiment_name)\n",
    "\n",
    "# Cargando la información\n",
    "client = mlflow.tracking.MlflowClient()\n",
    "experiment_id = client.get_experiment_by_name(experiment_name).experiment_id\n",
    "\n",
    "# Validacion\n",
    "print(f\"MLflow Version: {mlflow.__version__}\")\n",
    "print(f\"Tracking URI: {mlflow.tracking.get_tracking_uri()}\")\n",
    "print(f\"Nombre del experimento: {experiment_name}\")\n",
    "print(f\"ID del experimento: {experiment_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fecha = '20240526'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regresion Logistica"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Version Sencilla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "name = \"Logistic Regression_Simple_\"+fecha\n",
    "\n",
    "# Inicializa el modelo de Regresión Logística\n",
    "log_regression = LogisticRegression(max_iter=1000)\n",
    "\n",
    "# Iniciar un nuevo experimento en MLflow\n",
    "with mlflow.start_run(run_name=name):\n",
    "    # Entrena el modelo en los datos de entrenamiento\n",
    "    log_regression.fit(X_train, y_train)\n",
    "\n",
    "    # Realiza predicciones en el conjunto de prueba\n",
    "    y_pred = log_regression.predict(X_test)\n",
    "    y_prob = log_regression.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    # Evalúa el rendimiento del modelo\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    classification_rep = classification_report(y_test, y_pred, output_dict=True)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1= f1_score(y_test, y_pred)\n",
    "    log_loss = log_loss(y_test, y_prob)\n",
    "\n",
    "    # Registrar las métricas en MLflow\n",
    "    mlflow.log_metric(\"accuracy\", accuracy)\n",
    "    mlflow.log_metric(\"precision\", precision)\n",
    "    mlflow.log_metric(\"recall\", recall)\n",
    "    mlflow.log_metric(\"f1_score\", f1)\n",
    "    mlflow.log_metric(\"log_loss\", log_loss)\n",
    "\n",
    "\n",
    "    # Visualizar y guardar la matriz de confusión como artefacto\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(conf_matrix, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(np.unique(y_test)))\n",
    "    plt.xticks(tick_marks, np.unique(y_test))\n",
    "    plt.yticks(tick_marks, np.unique(y_test))\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"confusion_matrix-\" + name + \".png\")\n",
    "    mlflow.log_artifact(\"confusion_matrix-\" + name + \".png\")\n",
    "\n",
    "    # Registrar el informe de clasificación como artefacto\n",
    "    with open(\"classification_report-\"+name+\".json\", \"w\") as f:\n",
    "        json.dump(classification_rep, f)\n",
    "    mlflow.log_artifact(\"classification_report-\"+name+\".json\")\n",
    "\n",
    "    # Registrar el modelo en MLflow\n",
    "    mlflow.sklearn.log_model(log_regression, \"model-\"+name)\n",
    "\n",
    "    # Generar y registrar la curva de aprendizaje\n",
    "    train_sizes, train_scores, test_scores = learning_curve(log_regression, X_train, y_train, cv=5, n_jobs=-1, train_sizes=np.linspace(0.1, 1.0, 10))\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(train_sizes, train_scores_mean, label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, label=\"Cross-validation score\")\n",
    "    plt.xlabel(\"Training Examples\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.title(\"Learning Curve\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.grid()\n",
    "    plt.savefig(\"learning_curve-\"+name+\".png\")\n",
    "    mlflow.log_artifact(\"learning_curve-\"+name+\".png\")\n",
    "    \n",
    "    \n",
    "# Finaliza la ejecución\n",
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ajuste de hiperparamentros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "name = \"Logistic Regression-GridSearchCV_\"+fecha\n",
    "\n",
    "# Definir la cuadrícula de hiperparámetros a ajustar\n",
    "param_grid = {\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "    'class_weight': [None, 'balanced'],\n",
    "    'solver': ['liblinear']  # 'liblinear' supports l1 penalty\n",
    "}\n",
    "\n",
    "# Inicia una nueva ejecución en MLflow\n",
    "with mlflow.start_run(run_name=name):\n",
    "    # Inicializar el modelo de Regresión Logística\n",
    "    log_regression = LogisticRegression()\n",
    "\n",
    "    # Inicializar GridSearchCV\n",
    "    grid_search = GridSearchCV(log_regression, param_grid, cv=5, scoring='accuracy')\n",
    "\n",
    "    # Ajustar GridSearchCV en los datos de entrenamiento\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    # Mejores hiperparámetros\n",
    "    best_params = grid_search.best_params_\n",
    "\n",
    "    # Obtener el mejor modelo\n",
    "    best_model = grid_search.best_estimator_\n",
    "\n",
    "    # Evaluar el mejor modelo en los datos de prueba\n",
    "    y_pred_best_model = best_model.predict(X_test)\n",
    "    y_prob_best_model = best_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    # Accuracy del mejor modelo\n",
    "    accuracy_best_model = accuracy_score(y_test, y_pred_best_model)\n",
    "    \n",
    "    # Evalúa el rendimiento del modelo\n",
    "    accuracy = accuracy_score(y_test, y_pred_best_model)\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred_best_model)\n",
    "    classification_rep = classification_report(y_test, y_pred_best_model, output_dict=True)\n",
    "    precision = precision_score(y_test, y_pred_best_model)\n",
    "    recall = recall_score(y_test, y_pred_best_model)\n",
    "    f1= f1_score(y_test, y_pred_best_model)\n",
    "    log_loss = log_loss(y_test, y_prob_best_model)\n",
    "\n",
    "    # Registrar las métricas en MLflow\n",
    "    mlflow.log_metric(\"accuracy\", accuracy)\n",
    "    mlflow.log_metric(\"precision\", precision)\n",
    "    mlflow.log_metric(\"recall\", recall)\n",
    "    mlflow.log_metric(\"f1_score\", f1)\n",
    "    mlflow.log_metric(\"log_loss\", log_loss)\n",
    "    # Matriz de confusión\n",
    "    conf_matrix_best_model = confusion_matrix(y_test, y_pred_best_model)\n",
    "    \n",
    "    # Reporte de clasificación\n",
    "    classification_rep_best_model = classification_report(y_test, y_pred_best_model, output_dict=True)\n",
    "    # Registra los hiperparámetros en MLflow\n",
    "    mlflow.log_params(best_params)\n",
    "\n",
    "    # Registra las métricas en MLflow\n",
    "    mlflow.log_metric(\"accuracy\", accuracy_best_model)\n",
    "\n",
    "    # Guarda el modelo en MLflow\n",
    "    mlflow.sklearn.log_model(best_model, \"model_\"+name)\n",
    "\n",
    "    # Guarda la matriz de confusión como un artefacto\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    sns.heatmap(conf_matrix_best_model, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=np.unique(y_test), yticklabels=np.unique(y_test))\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.savefig(\"confusion_matrix-\"+name+\".png\")\n",
    "    mlflow.log_artifact(\"confusion_matrix-\"+name+\".png\")\n",
    "\n",
    "    # Genera los datos para la curva de aprendizaje\n",
    "    train_sizes, train_scores, test_scores = learning_curve(best_model, X_train, y_train, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "    # Calcula los promedios y desviaciones estándar de los puntajes de entrenamiento y prueba\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "\n",
    "    # Grafica la curva de aprendizaje\n",
    "    plt.figure()\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.1, color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\", label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\", label=\"Cross-validation score\")\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.title(\"Learning Curve\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.grid()\n",
    "    plt.savefig(\"learning_curve-\"+name+\".png\")\n",
    "    mlflow.log_artifact(\"learning_curve-\"+name+\".png\")\n",
    "\n",
    "\n",
    "# Finaliza la ejecución\n",
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Version Sencilla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "name = \"Decision Tree_Simple_\"+fecha\n",
    "\n",
    "\n",
    "with mlflow.start_run(run_name=name):\n",
    "    # Inicializar el modelo de árbol de decisión con los hiperparámetros especificados\n",
    "    decision_tree = DecisionTreeClassifier(max_depth=10, min_samples_split=100, min_samples_leaf=1000)\n",
    "\n",
    "    # Entrenar el modelo en los datos de entrenamiento\n",
    "    decision_tree.fit(X_train, y_train)\n",
    "\n",
    "    # Realizar predicciones en el conjunto de prueba\n",
    "    y_pred = decision_tree.predict(X_test)\n",
    "    y_prob = decision_tree.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    # Calcular y mostrar las métricas de evaluación\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    classification_rep = classification_report(y_test, y_pred, output_dict=True)\n",
    "\n",
    "    # Registra los hiperparámetros en MLflow\n",
    "    mlflow.log_param(\"max_depth\", 10)\n",
    "    mlflow.log_param(\"min_samples_split\", 100)\n",
    "    mlflow.log_param(\"min_samples_leaf\", 1000)\n",
    "\n",
    "    # Registra las métricas en MLflow\n",
    "    mlflow.log_metric(\"accuracy\", accuracy)\n",
    "\n",
    "    # Guarda el modelo en MLflow\n",
    "    mlflow.sklearn.log_model(decision_tree, \"model-\"+name)\n",
    "\n",
    "    # Guarda la matriz de confusión como un artefacto\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=np.unique(y_test), yticklabels=np.unique(y_test))\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.savefig(\"confusion_matrix-\"+name+\".png\")\n",
    "    mlflow.log_artifact(\"confusion_matrix-\"+name+\".png\")\n",
    "\n",
    "    # Genera los datos para la curva de aprendizaje\n",
    "    train_sizes, train_scores, test_scores = learning_curve(decision_tree, X_train, y_train, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "    # Calcula los promedios y desviaciones estándar de los puntajes de entrenamiento y prueba\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "\n",
    "    # Grafica la curva de aprendizaje\n",
    "    plt.figure()\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.1, color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\", label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\", label=\"Cross-validation score\")\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.title(\"Learning Curve\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.grid()\n",
    "    plt.savefig(\"learning_curve-\"+name+\".png\")\n",
    "    mlflow.log_artifact(\"learning_curve-\"+name+\".png\")\n",
    "\n",
    "\n",
    "# Finaliza la ejecución\n",
    "mlflow.end_run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ajuste de hiperparametros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "name = \"Decision Tree-GridSearchCV_\"+fecha\n",
    "\n",
    "param_grid = {\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'criterion': ['gini', 'entropy']\n",
    "}\n",
    "\n",
    "# Inicializar el modelo de árbol de decisión\n",
    "decision_tree = DecisionTreeClassifier()\n",
    "\n",
    "# Inicializar GridSearchCV\n",
    "grid_search = GridSearchCV(decision_tree, param_grid, cv=5, scoring='accuracy')\n",
    "\n",
    "# Inicia una nueva ejecución en MLflow\n",
    "name = \"Decision Tree Grid Search with Learning Curve and ROC\"\n",
    "with mlflow.start_run(run_name=name):\n",
    "    # Ajustar GridSearchCV en los datos de entrenamiento\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    # Obtener los mejores hiperparámetros\n",
    "    best_params = grid_search.best_params_\n",
    "    print(\"Mejores hiperparámetros:\", best_params)\n",
    "\n",
    "    # Obtener el mejor modelo\n",
    "    best_model = grid_search.best_estimator_\n",
    "\n",
    "    # Realizar predicciones en el conjunto de prueba con el mejor modelo\n",
    "    y_pred_best_model = best_model.predict(X_test)\n",
    "    y_prob_best_model = best_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    # Calcular y mostrar las métricas de evaluación\n",
    "    accuracy = accuracy_score(y_test, y_pred_best_model)\n",
    "    conf_matrix_best_model = confusion_matrix(y_test, y_pred_best_model)\n",
    "    classification_rep_best_model = classification_report(y_test, y_pred_best_model, output_dict=True)\n",
    "\n",
    "    # Registra los hiperparámetros en MLflow\n",
    "    mlflow.log_params(best_params)\n",
    "\n",
    "    # Registra las métricas en MLflow\n",
    "    mlflow.log_metric(\"accuracy\", accuracy)\n",
    "\n",
    "    # Guarda el modelo en MLflow\n",
    "    mlflow.sklearn.log_model(best_model, \"model-\"+name)\n",
    "\n",
    "    # Guarda la matriz de confusión como un artefacto\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    sns.heatmap(conf_matrix_best_model, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=np.unique(y_test), yticklabels=np.unique(y_test))\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.savefig(\"confusion_matrix-\"+name+\".png\")\n",
    "    mlflow.log_artifact(\"confusion_matrix-\"+name+\".png\")\n",
    "\n",
    "    # Genera los datos para la curva de aprendizaje\n",
    "    train_sizes, train_scores, test_scores = learning_curve(best_model, X_train, y_train, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "    # Calcula los promedios y desviaciones estándar de los puntajes de entrenamiento y prueba\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "\n",
    "    # Grafica la curva de aprendizaje\n",
    "    plt.figure()\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.1, color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\", label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\", label=\"Cross-validation score\")\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.title(\"Learning Curve\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.grid()\n",
    "    plt.savefig(\"learning_curve-\"+name+\".png\")\n",
    "    mlflow.log_artifact(\"learning_curve-\"+name+\".png\")\n",
    "\n",
    "\n",
    "\n",
    "# Finaliza la ejecución\n",
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Version Sencilla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "name = \"Random Forest_Simple_\"+fecha\n",
    "\n",
    "# Inicializar el clasificador de Random Forest\n",
    "rf_classifier = RandomForestClassifier(\n",
    "    n_estimators=100, \n",
    "    random_state=42, \n",
    "    max_depth=10, \n",
    "    min_samples_split=10, \n",
    "    min_samples_leaf=4\n",
    ")\n",
    "\n",
    "# Inicia una nueva ejecución en MLflow\n",
    "with mlflow.start_run(run_name=name):\n",
    "    # Entrenar el modelo\n",
    "    rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "    # Predecir en el conjunto de prueba\n",
    "    y_pred = rf_classifier.predict(X_test)\n",
    "    y_prob = rf_classifier.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    # Evaluar el modelo\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    classification_rep = classification_report(y_test, y_pred, output_dict=True)\n",
    "\n",
    "    # Registra los hiperparámetros en MLflow\n",
    "    mlflow.log_param(\"n_estimators\", 100)\n",
    "    mlflow.log_param(\"random_state\", 42)\n",
    "    mlflow.log_param(\"max_depth\", 10)\n",
    "    mlflow.log_param(\"min_samples_split\", 10)\n",
    "    mlflow.log_param(\"min_samples_leaf\", 4)\n",
    "\n",
    "    # Registra las métricas en MLflow\n",
    "    mlflow.log_metric(\"accuracy\", accuracy)\n",
    "\n",
    "    # Guarda el modelo en MLflow\n",
    "    mlflow.sklearn.log_model(rf_classifier, \"model-\"+name)\n",
    "\n",
    "    # Guarda la matriz de confusión como un artefacto\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=np.unique(y_test), yticklabels=np.unique(y_test))\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.savefig(\"confusion_matrix-\"+name+\".png\")\n",
    "    mlflow.log_artifact(\"confusion_matrix-\"+name+\".png\")\n",
    "\n",
    "    # Genera los datos para la curva de aprendizaje\n",
    "    train_sizes, train_scores, test_scores = learning_curve(rf_classifier, X_train, y_train, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "    # Calcula los promedios y desviaciones estándar de los puntajes de entrenamiento y prueba\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "\n",
    "    # Grafica la curva de aprendizaje\n",
    "    plt.figure()\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.1, color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\", label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\", label=\"Cross-validation score\")\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.title(\"Learning Curve\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.grid()\n",
    "    plt.savefig(\"learning_curve-\"+name+\".png\")\n",
    "    mlflow.log_artifact(\"learning_curve-\"+name+\".png\")\n",
    "\n",
    "\n",
    "# Finaliza la ejecución\n",
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ajuste de hiperparametros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"Random Forest_Grid Search_\"+fecha\n",
    "\n",
    "# Definir la cuadrícula de hiperparámetros a buscar\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],  # Número de árboles en el bosque\n",
    "    'max_depth': [None, 10, 20],  # Profundidad máxima de los árboles\n",
    "    'min_samples_split': [2, 5, 10],  # Número mínimo de muestras requeridas para dividir un nodo\n",
    "    'min_samples_leaf': [1, 2, 4]  # Número mínimo de muestras requeridas en cada hoja del árbol\n",
    "}\n",
    "\n",
    "# Inicializar el clasificador de Random Forest\n",
    "rf_classifier = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Inicializar la búsqueda de hiperparámetros utilizando validación cruzada de 5 pliegues\n",
    "grid_search = GridSearchCV(estimator=rf_classifier, param_grid=param_grid, cv=5, scoring='accuracy')\n",
    "\n",
    "# Inicia una nueva ejecución en MLflow\n",
    "with mlflow.start_run(run_name=name):\n",
    "    # Entrenar el modelo utilizando la búsqueda de hiperparámetros\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    # Mejores hiperparámetros\n",
    "    best_params = grid_search.best_params_\n",
    "\n",
    "    # Obtener el mejor modelo encontrado por la búsqueda de hiperparámetros\n",
    "    best_rf_classifier = grid_search.best_estimator_\n",
    "\n",
    "    # Predecir en el conjunto de prueba con el mejor modelo\n",
    "    y_pred_best_model = best_rf_classifier.predict(X_test)\n",
    "    y_prob_best_model = best_rf_classifier.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    # Accuracy del mejor modelo\n",
    "    accuracy_best_model = accuracy_score(y_test, y_pred_best_model)\n",
    "\n",
    "    # Matriz de confusión\n",
    "    conf_matrix_best_model = confusion_matrix(y_test, y_pred_best_model)\n",
    "\n",
    "    # Reporte de clasificación\n",
    "    classification_rep_best_model = classification_report(y_test, y_pred_best_model, output_dict=True)\n",
    "\n",
    "    # Registra los hiperparámetros en MLflow\n",
    "    mlflow.log_params(best_params)\n",
    "\n",
    "    # Registra las métricas en MLflow\n",
    "    mlflow.log_metric(\"accuracy\", accuracy_best_model)\n",
    "\n",
    "    # Guarda el modelo en MLflow\n",
    "    mlflow.sklearn.log_model(best_rf_classifier, \"model-\"+name)\n",
    "\n",
    "    # Guarda la matriz de confusión como un artefacto\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    sns.heatmap(conf_matrix_best_model, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=np.unique(y_test), yticklabels=np.unique(y_test))\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.savefig(\"confusion_matrix-\"+name+\".png\")\n",
    "    mlflow.log_artifact(\"confusion_matrix-\"+name+\".png\")\n",
    "\n",
    "    # Genera los datos para la curva de aprendizaje\n",
    "    train_sizes, train_scores, test_scores = learning_curve(best_rf_classifier, X_train, y_train, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "    # Calcula los promedios y desviaciones estándar de los puntajes de entrenamiento y prueba\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "\n",
    "    # Grafica la curva de aprendizaje\n",
    "    plt.figure()\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.1, color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\", label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\", label=\"Cross-validation score\")\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.title(\"Learning Curve\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.grid()\n",
    "    plt.savefig(\"learning_curve-\"+name+\".png\")\n",
    "    mlflow.log_artifact(\"learning_curve-\"+name+\".png\")\n",
    "\n",
    "\n",
    "# Finaliza la ejecución\n",
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Version Sencilla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "name = \"Random Forest_Simple_\"+fecha\n",
    "\n",
    "# Inicializar el clasificador de Random Forest\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42, max_depth=10, min_samples_split=10, min_samples_leaf=4)\n",
    "\n",
    "# Inicia una nueva ejecución en MLflow\n",
    "with mlflow.start_run() as run:\n",
    "    # Entrenar el modelo\n",
    "    rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "    # Predecir en el conjunto de prueba\n",
    "    y_pred = rf_classifier.predict(X_test)\n",
    "\n",
    "    # Evaluar el modelo\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    classification_rep = classification_report(y_test, y_pred)\n",
    "\n",
    "    # Guardar la matriz de confusión como una imagen\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=np.unique(y_test), yticklabels=np.unique(y_test))\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.savefig(\"confusion_matrix-\"+name+\".png\")\n",
    "    mlflow.log_artifact(\"confusion_matrix-\"+name+\".png\")\n",
    "\n",
    "    # Registra los hiperparámetros en MLflow\n",
    "    mlflow.log_params({\n",
    "        \"n_estimators\": rf_classifier.n_estimators,\n",
    "        \"max_depth\": rf_classifier.max_depth,\n",
    "        \"min_samples_split\": rf_classifier.min_samples_split,\n",
    "        \"min_samples_leaf\": rf_classifier.min_samples_leaf,\n",
    "        \"random_state\": rf_classifier.random_state,\n",
    "    })\n",
    "\n",
    "    # Registra las métricas en MLflow\n",
    "    mlflow.log_metric(\"accuracy\", accuracy)\n",
    "    mlflow.log_metric(\"precision\", classification_rep['weighted avg']['precision'])\n",
    "    mlflow.log_metric(\"recall\", classification_rep['weighted avg']['recall'])\n",
    "    mlflow.log_metric(\"f1-score\", classification_rep['weighted avg']['f1-score'])\n",
    "\n",
    "    # Genera los datos para la curva de aprendizaje\n",
    "    train_sizes, train_scores, test_scores = learning_curve(rf_classifier, X_train, y_train, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "    # Graficar la curva de aprendizaje\n",
    "    plt.figure()\n",
    "    plt.plot(train_sizes, np.mean(train_scores, axis=1), 'o-', label=\"Training score\")\n",
    "    plt.plot(train_sizes, np.mean(test_scores, axis=1), 'o-', label=\"Cross-validation score\")\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.title(\"Learning Curve\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.grid()\n",
    "    plt.savefig(\"learning_curve-\"+name+\".png\")\n",
    "    mlflow.log_artifact(\"learning_curve-\"+name+\".png\")\n",
    "\n",
    "    # Guarda el modelo en MLflow\n",
    "    mlflow.sklearn.log_model(rf_classifier, \"model-\"+name)\n",
    "    \n",
    "# Finalizar la ejecución\n",
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ajuste de hiperparametros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "name = \"Random Forest-GridSearchCV_\"+name\n",
    "\n",
    "# Inicializar el clasificador de Random Forest\n",
    "rf_classifier = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Definir la cuadrícula de hiperparámetros a buscar\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],  # Número de árboles en el bosque\n",
    "    'max_depth': [None, 10, 20],  # Profundidad máxima de los árboles\n",
    "    'min_samples_split': [2, 5, 10],  # Número mínimo de muestras requeridas para dividir un nodo\n",
    "    'min_samples_leaf': [1, 2, 4]  # Número mínimo de muestras requeridas en cada hoja del árbol\n",
    "}\n",
    "\n",
    "# Inicializar la búsqueda de hiperparámetros utilizando validación cruzada de 5 pliegues\n",
    "grid_search = GridSearchCV(estimator=rf_classifier, param_grid=param_grid, cv=5, scoring='accuracy')\n",
    "\n",
    "# Inicia una nueva ejecución en MLflow\n",
    "with mlflow.start_run(run_name=name) as run:\n",
    "    # Entrenar el modelo utilizando la búsqueda de hiperparámetros\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    # Mejores hiperparámetros\n",
    "    best_params = grid_search.best_params_\n",
    "    print(\"Mejores hiperparámetros:\", best_params)\n",
    "    mlflow.log_params(best_params)\n",
    "\n",
    "    # Obtener el mejor modelo encontrado por la búsqueda de hiperparámetros\n",
    "    best_rf_classifier = grid_search.best_estimator_\n",
    "\n",
    "    # Predecir en el conjunto de prueba con el mejor modelo\n",
    "    y_pred_best_model = best_rf_classifier.predict(X_test)\n",
    "\n",
    "    # Evaluar el modelo\n",
    "    accuracy_best_model = accuracy_score(y_test, y_pred_best_model)\n",
    "\n",
    "    # Matriz de confusión\n",
    "    conf_matrix_best_model = confusion_matrix(y_test, y_pred_best_model)\n",
    "\n",
    "    # Reporte de clasificación\n",
    "    classification_rep_best_model = classification_report(y_test, y_pred_best_model)\n",
    "\n",
    "    # Guardar la matriz de confusión como una imagen\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(conf_matrix_best_model, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=np.unique(y_test), yticklabels=np.unique(y_test))\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.savefig(\"confusion_matrix_\"+name+\".png\")\n",
    "    mlflow.log_artifact(\"confusion_matrix_\"+name+\".png\")\n",
    "\n",
    "    # Generar los datos para la curva de aprendizaje\n",
    "    train_sizes, train_scores, test_scores = learning_curve(best_rf_classifier, X_train, y_train, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "    # Graficar la curva de aprendizaje\n",
    "    plt.figure()\n",
    "    plt.plot(train_sizes, np.mean(train_scores, axis=1), 'o-', label=\"Training score\")\n",
    "    plt.plot(train_sizes, np.mean(test_scores, axis=1), 'o-', label=\"Cross-validation score\")\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.title(\"Learning Curve\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.grid()\n",
    "    plt.savefig(\"learning_curve.png\")\n",
    "    mlflow.log_artifact(\"learning_curve.png\")\n",
    "\n",
    "    # Registra las métricas en MLflow\n",
    "    mlflow.log_metric(\"accuracy\", accuracy_best_model)\n",
    "    mlflow.log_metric(\"precision\", classification_rep_best_model['weighted avg']['precision'])\n",
    "    mlflow.log_metric(\"recall\", classification_rep_best_model['weighted avg']['recall'])\n",
    "    mlflow.log_metric(\"f1-score\", classification_rep_best_model['weighted avg']['f1-score'])\n",
    "\n",
    "    # Guardar el modelo en MLflow\n",
    "    mlflow.sklearn.log_model(best_rf_classifier, \"random_forest_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Version Sencilla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "name = \"GradientBoostingClassifier_Simple_\"+fecha\n",
    "\n",
    "# Inicializar el modelo de Gradient Boosting\n",
    "gradient_boosting = GradientBoostingClassifier()\n",
    "\n",
    "# Iniciar un nuevo experimento en MLflow\n",
    "with mlflow.start_run(run_name=name):\n",
    "    # Entrenar el modelo en los datos de entrenamiento\n",
    "    gradient_boosting.fit(X_train, y_train)\n",
    "    \n",
    "    # Realizar predicciones en el conjunto de prueba\n",
    "    y_pred = gradient_boosting.predict(X_test)\n",
    "    y_prob = gradient_boosting.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "     # Evalúa el rendimiento del modelo\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    classification_rep = classification_report(y_test, y_pred, output_dict=True)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1= f1_score(y_test, y_pred)\n",
    "    log_loss = log_loss(y_test, y_prob)\n",
    "\n",
    "    # Registrar las métricas en MLflow\n",
    "    mlflow.log_metric(\"accuracy\", accuracy)\n",
    "    mlflow.log_metric(\"precision\", precision)\n",
    "    mlflow.log_metric(\"recall\", recall)\n",
    "    mlflow.log_metric(\"f1_score\", f1)\n",
    "    mlflow.log_metric(\"log_loss\", log_loss)\n",
    "    \n",
    "    # Guarda la matriz de confusión como un artefacto\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=np.unique(y_test), yticklabels=np.unique(y_test))\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.savefig(\"confusion_matrix-\"+name+\".png\")\n",
    "    mlflow.log_artifact(\"confusion_matrix-\"+name+\".png\")\n",
    "    \n",
    "    \n",
    "    # Registrar el informe de clasificación como artefacto\n",
    "    with open(\"classification_report-\"+name+\".json\", \"w\") as f:\n",
    "        json.dump(classification_rep, f)\n",
    "    mlflow.log_artifact(\"classification_report-\"+name+\".json\")\n",
    "    \n",
    "    # Registrar el modelo en MLflow\n",
    "    mlflow.sklearn.log_model(gradient_boosting, \"model-\"+name)\n",
    "    \n",
    "    # Generar y registrar la curva de aprendizaje\n",
    "    train_sizes, train_scores, test_scores = learning_curve(gradient_boosting, X_train, y_train, cv=5, n_jobs=-1, train_sizes=np.linspace(0.1, 1.0, 10))\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot(train_sizes, train_scores_mean, label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, label=\"Cross-validation score\")\n",
    "    plt.xlabel(\"Training Examples\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.title(\"Learning Curve\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.grid()\n",
    "    plt.savefig(\"learning_curve-\"+name+\".png\")\n",
    "    mlflow.log_artifact(\"learning_curve-\"+name+\".png\")\n",
    "    \n",
    "\n",
    "# Finalizar la ejecución\n",
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ajuste de hiperparametros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "name = \"GradientBoosting-GridSearchCV_\"+fecha\n",
    "\n",
    "# Definir la cuadrícula de hiperparámetros a ajustar\n",
    "param_grid = {\n",
    "    'learning_rate': [0.05, 0.1, 0.2],\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Inicializar el modelo de Gradient Boosting\n",
    "gradient_boosting = GradientBoostingClassifier()\n",
    "\n",
    "# Inicializar GridSearchCV\n",
    "grid_search = GridSearchCV(gradient_boosting, param_grid, cv=5, scoring='accuracy')\n",
    "\n",
    "# Iniciar un nuevo experimento en MLflow\n",
    "with mlflow.start_run(run_name=name):\n",
    "    # Ajustar GridSearchCV en los datos de entrenamiento\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Obtener los mejores hiperparámetros\n",
    "    best_params = grid_search.best_params_\n",
    "    mlflow.log_params(best_params)\n",
    "    print(\"Mejores hiperparámetros:\", best_params)\n",
    "    \n",
    "    # Obtener el mejor modelo encontrado por la búsqueda de hiperparámetros\n",
    "    best_rf_classifier = grid_search.best_estimator_\n",
    "\n",
    "    # Predecir en el conjunto de prueba con el mejor modelo\n",
    "    y_pred_best_model = best_rf_classifier.predict(X_test)\n",
    "    y_prob_best_model = best_rf_classifier.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Calcula las metricas\n",
    "    accuracy_best_model = accuracy_score(y_test, y_pred_best_model)\n",
    "    precision_best_model = precision_score(y_test, y_pred_best_model)\n",
    "    recall_best_model = recall_score(y_test, y_pred_best_model)\n",
    "    f1_best_model = f1_score(y_test, y_pred_best_model)\n",
    "    log_loss_best_model = log_loss(y_test, y_prob_best_model)\n",
    "\n",
    "    # Registra las métricas en MLflow\n",
    "    mlflow.log_metric(\"accuracy\", accuracy_best_model)\n",
    "    mlflow.log_metric(\"precision\", precision_best_model)\n",
    "    mlflow.log_metric(\"recall\", recall_best_model)\n",
    "    mlflow.log_metric(\"f1_score\", f1_best_model)\n",
    "    mlflow.log_metric(\"log_loss\", log_loss_best_model)\n",
    "    \n",
    "    # Matriz de confusión\n",
    "    conf_matrix_best_model = confusion_matrix(y_test, y_pred_best_model)\n",
    "    # Guarda la matriz de confusión como un artefacto\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    sns.heatmap(conf_matrix_best_model, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=np.unique(y_test), yticklabels=np.unique(y_test))\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.savefig(\"confusion_matrix-\"+name+\".png\")\n",
    "    mlflow.log_artifact(\"confusion_matrix-\"+name+\".png\")\n",
    "\n",
    "    # Reporte de clasificación\n",
    "    classification_rep_best_model = classification_report(y_test, y_pred_best_model, output_dict=True)\n",
    "    with open(\"classification_report-\"+name+\".json\", \"w\") as f:\n",
    "        json.dump(classification_rep_best_model, f)\n",
    "    mlflow.log_artifact(\"classification_report-\"+name+\".json\")\n",
    "\n",
    "    # Generar y registrar la curva de aprendizaje\n",
    "    train_sizes, train_scores, test_scores = learning_curve(best_rf_classifier, X_train, y_train, cv=5, n_jobs=-1, train_sizes=np.linspace(0.1, 1.0, 10))\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(train_sizes, train_scores_mean, label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, label=\"Cross-validation score\")\n",
    "    plt.xlabel(\"Training Examples\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.title(\"Learning Curve\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.grid()\n",
    "    plt.savefig(\"learning_curve-\"+name+\".png\")\n",
    "    mlflow.log_artifact(\"learning_curve-\"+name+\".png\")\n",
    "\n",
    "# Finalizar la ejecución\n",
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
