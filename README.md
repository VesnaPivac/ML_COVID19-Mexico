# Proyecto de Machine Learning con datos de COVID-19 M√©xico

Este proyecto forma parte de la materia de Machine Learning de la Maestr√≠a en Ciencia de Datos de la Universidad de Sonora. Se utilizan los datos abiertos del gobierno de M√©xico sobre los casos de COVID-19. Estos datos pueden encontrarse en la siguiente liga: [Informaci√≥n sobre casos de COVID-19 en M√©xico](https://datos.gob.mx/busca/dataset/informacion-referente-a-casos-covid-19-en-mexico).

### Integrantes del equipo

- Jos√© Barreras
- Luis Ortiz
- Vesna Pivac

## ¬øQue problema se plantea resolver?

Utilizando los datos abiertos publicados por el gobierno de M√©xico acerca de los casos de COVID-19, se busca identificar qu√© pacientes tienen un mayor riesgo de fallecer. Se pretende entrenar un modelo de Aprendizaje Autom√°tico que pueda predecir si un paciente fallecer√° o no bas√°ndose en sus caracter√≠sticas m√©dicas y demogr√°ficas disponibles.

El an√°lisis exploratorio de los datos, junto con las pruebas de entrenamiento y validaci√≥n de los modelos, promete desvelar qu√© variables son las m√°s influyentes a la hora de predecir el riesgo de defunci√≥n. Esto proporcionar√° informaci√≥n valiosa para futuras intervenciones y pol√≠ticas de salud p√∫blica. 

## ¬øPorqu√© es un problema importante para la instituci√≥n/organizaci√≥n/empresa?

Este problema es crucial para M√©xico porque permite salvaguardar vidas al identificar a los pacientes con mayor riesgo de fallecer, lo que permite a los profesionales de la salud priorizar cuidados intensivos y tratamientos m√°s agresivos para quienes m√°s lo necesiten. Adem√°s, en un contexto de recursos limitados como camas de hospital, ventiladores y personal m√©dico, dirigir estos recursos hacia los pacientes en mayor peligro asegura un uso m√°s eficiente y eficaz.

Un modelo predictivo tambi√©n facilita una mejor planificaci√≥n y una respuesta r√°pida a los picos de la pandemia, preparando con antelaci√≥n los recursos necesarios y evitando colapsos en el sistema de salud. Al identificar los factores que influyen en la probabilidad de defunci√≥n, se pueden dise√±ar pol√≠ticas de salud m√°s efectivas y campa√±as de prevenci√≥n dirigidas a las poblaciones m√°s vulnerables.

En el √°mbito pol√≠tico, mostrar que se est√°n tomando medidas basadas en datos e investigaci√≥n para combatir la crisis del COVID-19 puede aumentar la confianza p√∫blica en las decisiones del gobierno y en las medidas de salud implementadas, fomentando una mayor cooperaci√≥n y cumplimiento por parte de la ciudadan√≠a.

## ¬øCuales son las m√©tricas para medir el impacto de la soluci√≥n una vez obtenida?

Para evaluar el impacto de la soluci√≥n una vez implementada a nivel nacional, se pueden utilizar diversas m√©tricas que reflejan los resultados en el sistema de salud. Un indicador clave es el cambio en la tasa de defunciones por COVID-19, comparando las cifras antes y despu√©s de la implementaci√≥n del modelo. Una disminuci√≥n en esta tasa indicar√≠a que el modelo est√° ayudando a identificar y tratar adecuadamente a los pacientes en mayor riesgo.

Otro aspecto importante es la eficiencia en la utilizaci√≥n de recursos. La reducci√≥n en la saturaci√≥n de hospitales, unidades de cuidados intensivos y el uso de ventiladores ser√≠a un claro indicativo de una mejor asignaci√≥n de recursos gracias al modelo predictivo. Asimismo, el tiempo de respuesta en la atenci√≥n de pacientes cr√≠ticos podr√≠a ser evaluado para determinar si la soluci√≥n permite una intervenci√≥n m√°s r√°pida y efectiva.

La calidad de la atenci√≥n m√©dica tambi√©n puede ser medida a trav√©s de la tasa de recuperaci√≥n de pacientes, esperando un aumento en esta tasa debido a un tratamiento m√°s adecuado y temprano para aquellos identificados como de alto riesgo. La satisfacci√≥n de los pacientes y sus familias con la atenci√≥n recibida es otro aspecto fundamental, especialmente en t√©rminos de confianza y percepci√≥n de la efectividad del tratamiento.

Por √∫ltimo, la implementaci√≥n y adopci√≥n del modelo en el sistema de salud se pueden evaluar mediante el porcentaje de instituciones que han adoptado el modelo y lo utilizan en su pr√°ctica diaria, as√≠ como la capacitaci√≥n del personal m√©dico en el uso del modelo y su integraci√≥n en los procesos de toma de decisiones cl√≠nicas. Estas m√©tricas, en conjunto, permiten evaluar el impacto pr√°ctico y beneficioso de la soluci√≥n en el sistema de salud, contribuyendo a una mejor atenci√≥n de los pacientes y optimizaci√≥n de los recursos disponibles.

## ¬øQue problema de aprendizaje implica resolver?

El problema que se busca resolver es un problema de **clasificaci√≥n binaria**. En t√©rminos de aprendizaje autom√°tico, esto significa que el objetivo es asignar a cada paciente uno de dos posibles resultados:

**Clase 1:** El paciente falleci√≥.

**Clase 2:** El paciente no falleci√≥.

El modelo debe aprender a partir de los datos hist√≥ricos de pacientes con COVID-19 para predecir, bas√°ndose en las caracter√≠sticas m√©dicas y demogr√°ficas disponibles, a cu√°l de estas dos clases pertenece un nuevo paciente.

## ¬øQu√© metricas permiten medir la calidad del modelo de aprendizaje? ¬øCuales son sus valores deseables?

Dado que nuestros datos est√°n altamente desbalanceados, con un 99% de pacientes no fallecidos y solo un 1% de fallecidos, es crucial elegir m√©tricas que reflejen adecuadamente el rendimiento del modelo en ambos grupos. Las m√©tricas tradicionales como el *Accuracy* (Exactitud) no son adecuadas en este contexto, ya que un modelo que prediga siempre "no falleci√≥", dado el predominio de esta clase, tendr√≠a un alto *Accuracy* pero no ser√≠a √∫til. En su lugar, seleccionamos m√©tricas que nos permiten evaluar la capacidad del modelo para identificar correctamente a los pacientes fallecidos sin perder de vista el contexto de clases desbalanceadas. A continuaci√≥n se detallan las m√©tricas clave y sus valores deseables:

**Precision (Precisi√≥n):** es el n√∫mero de verdaderos positivos dividido por el n√∫mero total de predicciones positivas. En otras palabras, es una medida de cu√°ntas de las predicciones positivas del modelo fueron correctas. Un valor de precision alto es deseable para minimizar los falsos positivos. En este caso, tal como en muchos otros contextos, consideraremos como buenos valores superiores a 0.75.

**Recall (Sensibilidad o Tasa de Verdaderos Positivos):** es el n√∫mero de verdaderos positivos dividido por el n√∫mero total de positivos reales. Mide la capacidad del modelo para identificar correctamente los casos positivos. Un Recall alto es crucial para este problema, ya que es importante identificar la mayor cantidad posible de casos de fallecimiento. Consideraremos haber logrado un buen Recall si obtenemos valores por encima de 0.6. 

**F1-Score:** es la media arm√≥nica de la Precision y el Recall, y es particularmente √∫til cuando hay un desbalance de clases, como en este caso donde los fallecimientos son una minor√≠a. Un valor cercano a 1 indica un buen equilibrio entre precisi√≥n y recall. Consideraremos que valores mayores a 0.75 denotar√°n un buen F1-score. 

**ROC-AUC (√Årea Bajo la Curva de la Caracter√≠stica Operativa del Receptor):** la curva ROC muestra la tasa de verdaderos positivos frente a la tasa de falsos positivos a diferentes umbrales. El AUC mide el √°rea bajo esta curva. Mientras que un AUC cercano a 1 indica un excelente rendimiento del modelo, en nuestro presente problema de clasificaci√≥n consideraremos buenos valores superiores a 0.85.

**AUC-PR (√Årea Bajo la Curva de Precisi√≥n-Recall):** similar al ROC-AUC, pero m√°s informativo en el contexto de clases desbalanceadas. Aqu√≠ tambi√©n un valor cercano a 1 es ideal, pero nosotros consideraremos valores superiores a 0.75 como buenos.

**Matriz de Confusi√≥n:** muestra los verdaderos positivos, verdaderos negativos, falsos positivos y falsos negativos. Aunque no fijamos proporciones espec√≠ficas para cada una de estas categor√≠as, esperamos que la matriz muestre un alto n√∫mero de verdaderos positivos y verdaderos negativos, y un bajo n√∫mero de falsos positivos y falsos negativos.

## ¬øComo est√°n alineadas las m√©tricas de la calidad del modelo con las m√©tricas de impacto de la soluci√≥n?

Un valor de *Precision* alto significa que la mayor√≠a de las predicciones positivas son correctas. Esto reduce los falsos positivos, evitando que recursos limitados se asignen a pacientes que no los necesitan. Mejorar la *Precision* podr√≠a resultar en una disminuci√≥n significativa en la saturaci√≥n de hospitales, por ejemplo, reduciendo la ocupaci√≥n innecesaria de camas y ventiladores.

Un valor alto de *Recall* asegura que la mayor√≠a de los casos positivos sean identificados, minimizando los falsos negativos. Esto es crucial para garantizar que los pacientes en riesgo reciban la atenci√≥n necesaria. Un incremento del *Recall* podr√≠a correlacionarse con una mejora en la tasa de recuperaci√≥n de pacientes, debido a la detecci√≥n temprana y tratamiento de casos cr√≠ticos.

El resto de las m√©tricas de calidad del modelo (F1-Score, ROC-AUC, AUC-PR) est√°n alineadas con las m√©tricas de impacto de la soluci√≥n a nivel nacional, ya que todas contribuyen a mejorar el rendimiento del modelo, lo que se traduce en un mejor funcionamiento del sistema de salud. Sin embargo, cada m√©trica tambi√©n tiene un aporte espec√≠fico. Un alto *F1-Score* indica un buen equilibrio entre precisi√≥n y recall, mejorando la confianza en las decisiones cl√≠nicas y asegurando que los recursos se dirijan efectivamente a los pacientes en mayor riesgo sin sobrecargar el sistema de salud con falsos positivos. El *ROC-AUC* demuestra la capacidad del modelo para distinguir entre pacientes fallecidos y no fallecidos en un rango amplio de umbrales, facilitando la flexibilidad en la toma de decisiones cl√≠nicas y permitiendo ajustar umbrales seg√∫n la disponibilidad de recursos y la situaci√≥n epidemiol√≥gica. Un alto *AUC-PR* es especialmente importante en contextos de desbalance de clases, asegurando que el modelo sea robusto en detectar los casos cr√≠ticos, lo cual es vital para priorizar la atenci√≥n y recursos a los pacientes m√°s vulnerables.

## Repositorios
- [Github](https://github.com/VesnaPivac/ML_COVID19-Mexico)
- [Dagshub](https://dagshub.com/VesnaPivac/ML_COVID19-Mexico)

## Instructions
1. Clone the repo.
2. Run `make dirs` to create the missing parts of the directory structure described below.
3. *Optional:* Run `make virtualenv` to create a python virtual environment. Skip if using conda or some other env manager.
   1. Run `source env/bin/activate` to activate the virtualenv.
4. Run `make requirements` to install required python packages.
5. Put the raw data in `data/raw`.
6. To save the raw data to the DVC cache, run `dvc add data/raw`
7. Edit the code files to your heart's desire.
8. Process your data, train and evaluate your model using `dvc repro` or `make reproduce`
9. To run the pre-commit hooks, run `make pre-commit-install`
10. For setting up data validation tests, run `make setup-setup-data-validation`
11. For **running** the data validation tests, run `make run-data-validation`
12. When you're happy with the result, commit files (including .dvc files) to git.

## Project Organization

    ‚îú‚îÄ‚îÄ LICENSE
    ‚îú‚îÄ‚îÄ Makefile           <- Makefile with commands like `make dirs` or `make clean`
    ‚îú‚îÄ‚îÄ README.md          <- The top-level README for developers using this project.
    ‚îú‚îÄ‚îÄ data
    ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ processed      <- The final, canonical data sets for modeling.
    ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ raw.dvc        <- DVC file that tracks the raw data
    ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ raw            <- The original, immutable data dump
    ‚îÇ
    ‚îú‚îÄ‚îÄ models             <- Trained and serialized models, model predictions, or model summaries
    ‚îÇ
    ‚îú‚îÄ‚îÄ references         <- Data dictionaries, manuals, and all other explanatory materials.
    ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ processed      <- Data dictionaries, manuals, and all other explanatory materials of processed data
    ‚îÇ¬†¬†     ‚îî‚îÄ‚îÄ Diccionario.xlsx    <- Processed Data dictionary
    ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ raw            <- Data dictionaries, manuals, and all other explanatory materials of raw data
    ‚îÇ¬†¬†     ‚îú‚îÄ‚îÄ Catalogos.xlsx      <- Raw Data catalog
    ‚îÇ¬†¬†     ‚îú‚îÄ‚îÄ Diccionario.xlsx    <- Raw Data dictionary
    ‚îÇ¬†¬†     ‚îî‚îÄ‚îÄ Actualizaciones en la presentaci√≥n de informaci√≥n referente a casos de COVID.pdf <- Info of raw data
    ‚îú‚îÄ‚îÄ reports            <- Generated analysis as HTML, PDF, LaTeX, etc.
    ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ figures        <- Generated graphics and figures to be used in reporting
    ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ metrics.txt    <- Relevant metrics after evaluating the model.
    ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ training_metrics.txt    <- Relevant metrics from training the model.
    ‚îÇ
    ‚îú‚îÄ‚îÄ requirements.txt   <- The requirements file for reproducing the analysis environment, e.g.
    ‚îÇ                         generated with `pip freeze > requirements.txt`
    ‚îÇ
    ‚îú‚îÄ‚îÄ setup.py           <- Makes project pip installable (pip install -e .) so src can be imported
    ‚îú‚îÄ‚îÄ src                <- Source code for use in this project.
    ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ __init__.py    <- Makes src a Python module
    ‚îÇ   ‚îÇ
    ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ features           <- Scripts to download or generate data
    ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ build_features.ipynb  <- Notebook that cleans and processes raw data
    ‚îÇ   ‚îÇ
    ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ data           <- Scripts to download or generate data
    ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ Descarga_de_datos.ipynb  <- Notebook that downloads raw data
    ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ make_dataset.py
    ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ data_validation.py  <- Script to run data integrity checks
    ‚îÇ   ‚îÇ
    ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ models         <- Scripts to train models and then use trained models to make
    ‚îÇ   ‚îÇ   ‚îÇ                 predictions
    ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ predict_model.py
    ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ train_model.py
    ‚îÇ   ‚îÇ
    ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ visualization  <- Scripts to create exploratory and results oriented visualizations
    ‚îÇ¬†¬†     ‚îî‚îÄ‚îÄ visualize.py
    ‚îÇ
    ‚îú‚îÄ‚îÄ .pre-commit-config.yaml  <- pre-commit hooks file with selected hooks for the projects.
    ‚îú‚îÄ‚îÄ dvc.lock           <- The version definition of each dependency, stage, and output from the 
    ‚îÇ                         data pipeline.
    ‚îî‚îÄ‚îÄ dvc.yaml           <- Defining the data pipeline stages, dependencies, and outputs.


--------

<p><small>Project based on the <a target="_blank" href="https://drivendata.github.io/cookiecutter-data-science/">cookiecutter data science project template</a>. #cookiecutterdatascience</small></p>


---

To create a project like this, just go to https://dagshub.com/repo/create and select the **Cookiecutter DVC** project template.

Made with üê∂ by [DAGsHub](https://dagshub.com/).
